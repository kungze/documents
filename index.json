[{"content":"kolla-helm 包含一些列 helm chart，基于这些 chart 我们可以很容易的在 k8s 平台上部署 openstack，在 openstack 社区有一个 openstack-helm 项目 提供了类似的功能，但是由于 openstack-helm 设计过于复杂，项目活跃度底，不兼容最新的 helm v3，所以我们打算提供一套新的 chart。 我们为什么叫 kolla-helm 呢？顾名思义，我们想要借助 kolla 容器来完成新版 chart 的编写。 借助这些 chart 你可以有多种方式来轻松的部署 openstack\n通过命令行部署 你可以通过 helm 命令部署 openstack。\n图形化部署 你还可以通过 kubeapp 提供的 web UI 轻松的部署 openstack。\n该目录下面的文档时以命令行部署的方式展示如何通过 kolla-helm 部署 openstack。\n","description":"","tags":null,"title":"kolla-helm","uri":"/documents/kolla-helm/"},{"content":"部署 k8s 首先我们需要有一套 k8s 集群，k8s 集群可以有多种方式，由于国内网络环境的问题，我们推荐两个可以在国内丝滑部署 k8s 集群的方案\n kubeadm，可以参考我们的文档在国内使用 kubeadm 部署 k8s kubeasz，这是由国内大神编写的一套部署 k8s 的 ansible 脚本，可以满足各种需要，使用方式可以参考这个项目的文档  需要特别注意的是：为了能在宿主机节点上解析 k8s service 的域名（方便在宿主机上执行 openstack 命令）， nodelocaldns 是强烈推荐安装的。\n部署 metallb 我们提供的方案需要在裸金属上直接部署 k8s，所以我们需要部署 metallb 为 loadbalancer 类型的 service 提供支撑，部署方式可以参考文档。 另外 openelb 也是一个不错的方案，但是 openelb 作者没有部署过，感兴趣的可以自己找文档研究部署。\n部署 rook（可选） rook 是一个开源的云原生存储编排平台，可以用来管理 ceph 集群，当我们 设置 cinder，glance，nova 的后端存储为 ceph 时，我们先必须要部署 rook，并使用 rook 创建创建一个 ceph 集群 或者纳管一个外部的 ceph 集群。\nceph 集群（可选） 我们可以参考官方文档 使用 rook 创建 一套 ceph 集群，但是在生产环境中我们更推荐使用传统先行部署一套 ceph 集群，然后再用 rook 纳管已经部署好的 ceph 集群。ceph 集群的部署可以使用 ceph-ansible，rook 纳管已经存在的 ceph 集群的方法可以参考文档.\n安装 helm helm 的安装就比较简单了，可以参考官方文档，而且只用在执行部署命令的节点安装就行。 另外还可以安装 kubeapps 来为我们提供图形界面部署 openstack。kubeapps 的安装 文档可以参考我们专门写的一篇博客。\n添加 helm 仓库 helm repo add kolla-helm https://kungze.github.io/kolla-helm 在国内访问 github 可能会失败，你可以使用我们在国内的备份仓库\nhelm repo add kolla-helm https://charts.kungze.net 创建 namespace（可选） 建议单独创建一个 k8s namespace 用于部署 openstack 相关的 chart。\nkubectl create namespace openstack 在准备工作完成后我们可以正式部署 openstack 了，注意：password，openstack-dep 和 keystone 需要优先部署。\n","description":"","tags":null,"title":"准备工作","uri":"/documents/kolla-helm/preparation/"},{"content":"kubeadm 是 k8s 官方提供的一个部署管理 k8s 集群的工具，但是 kubeadm 使用到的很多资源的下载地址都在国外，如果按照官方文档操作很容易因为网络原因失败。这里基于 ubuntu 20.04 展示如何让 kubeadm 使用国内的资源部署 k8s 集群。\n下面所有命令都是使用 root 用户执行的\n安装 docker 参照 docker 官方安装文档\n$ curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo gpg --dearmor -o /usr/share/keyrings/docker-archive-keyring.gpg $ echo \\ \"deb [arch=$(dpkg --print-architecture) signed-by=/usr/share/keyrings/docker-archive-keyring.gpg] https://download.docker.com/linux/ubuntu \\ $(lsb_release -cs) stable\" | sudo tee /etc/apt/sources.list.d/docker.list \u003e /dev/null $ apt-get update $ apt-get install docker-ce docker-ce-cli containerd.io 通过阿里源安装：\n$ curl -fsSL https://mirrors.aliyun.com/docker-ce/linux/ubuntu/gpg | sudo gpg --dearmor -o /usr/share/keyrings/docker-archive-keyring.gpg $ echo \\ \"deb [arch=amd64 signed-by=/usr/share/keyrings/docker-archive-keyring.gpg] https://mirrors.aliyun.com/docker-ce/linux/ubuntu \\ $(lsb_release -cs) stable\" | sudo tee /etc/apt/sources.list.d/docker.list \u003e /dev/null $ apt-get update $ apt-get install docker-ce docker-ce-cli containerd.io 安装 kubeadm 官方文档使用的 https://apt.kubernetes.io/ apt 源在国内被屏蔽了，所以我们需要找一个国内的镜像源，这里我们以阿里源为例\napt-get install -y apt-transport-https curl https://mirrors.aliyun.com/kubernetes/apt/doc/apt-key.gpg | apt-key add - cat \u003c\u003cEOF \u003e/etc/apt/sources.list.d/kubernetes.list deb https://mirrors.aliyun.com/kubernetes/apt/ kubernetes-xenial main EOF apt update 在安装之前可以通过下面命令我们可以安装的 kubeadm 的版本\napt-cache madison kubeadm 根据情况选择适合的版本安装\napt-get install kubectl=1.23.7-00 kubelet=1.23.7-00 kubeadm=1.23.7-00 注意：这里选择的版本决定了最终安装的 k8s 的版本，从 1.24 开始 k8s 从代码中彻底移除了 dockershim，所以我们这里选择用 k8s 1.23.7，如果想在 k8s 1.24 及以后使用 docker 作为 cri，则需要安装一个外部的 dockershim，mirantis 的 cri-dockerd 是一个不错的选择。\n节点准备 关闭 swap 分区 ( 貌似最新的 k8s 版本不在要求这一步 )\nsudo swapoff -a # 暂时关闭，永久关闭可以上网查询 初始化环境 创建一个 yaml 文件，我们命名为 kubeadm-init-config.yaml，填入以下内容\n---apiVersion:kubeadm.k8s.io/v1beta3kind:InitConfigurationbootstrapTokens:- token:abcdef.0123456789abcdefttl:24h0m0slocalAPIEndpoint:advertiseAddress:172.18.30.127bindPort:6443nodeRegistration:criSocket:/var/run/dockershim.sockimagePullPolicy:IfNotPresenttaints:[]---apiVersion:kubeadm.k8s.io/v1beta3kind:ClusterConfigurationapiServer:timeoutForControlPlane:4m0scertificatesDir:/etc/kubernetes/pkiclusterName:kubernetescontrollerManager:{}dns:{}etcd:local:dataDir:/var/lib/etcdimageRepository:registry.aliyuncs.com/google_containerskubernetesVersion:1.23.0networking:dnsDomain:cluster.localserviceSubnet:10.96.0.0/12podSubnet:10.244.0.0/16scheduler:{}---apiVersion:kubelet.config.k8s.io/v1beta1kind:KubeletConfigurationfailSwapOn:falseaddress:0.0.0.0enableServer:truecgroupDriver:cgroupfs---apiVersion:kubeproxy.config.k8s.io/v1alpha1kind:KubeProxyConfigurationmode:ipvsipvs:strictARP:true这里有几个参数要特别注意一下：\n advertiseAddress：这个要改为当前主机的管理网 IP 地址 imageRepository：这是一个关键的配置，从国内源下载相关镜像 podSubnet: 这个是下面部署的 flannel cni 插件要求的一个参数，需要和 flannel 网络配置 net-conf.json 中的 Network 保持一致 cgroupDriver: 这个需要和 docker 的 Cgroup Driver 保持一致，可以通过命令 docker info|grep \"Cgroup Driver\" 查看  然后执行：\nsudo kubeadm init --config kubeadm-init-config.yaml 如果你不想关闭 swap 分区，使用下面命令初始化\nsudo kubeadm init --config kubeadm-init-config.yaml --ignore-preflight-errors Swap 等待一会儿，初始化成功后会获得如下输出：\nYour Kubernetes control-plane has initialized successfully! To start using your cluster, you need to run the following as a regular user: mkdir -p $HOME/.kube sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config sudo chown $(id -u):$(id -g) $HOME/.kube/config Alternatively, if you are the root user, you can run: export KUBECONFIG=/etc/kubernetes/admin.conf You should now deploy a pod network to the cluster. Run \"kubectl apply -f [podnetwork].yaml\" with one of the options listed at: https://kubernetes.io/docs/concepts/cluster-administration/addons/ Then you can join any number of worker nodes by running the following on each as root: kubeadm join 172.18.30.127:6443 --token abcdef.0123456789abcdef \\ --discovery-token-ca-cert-hash sha256:159072d62927901183f5cda29fbb4e110ee8e354a350aad7774239e19c57169a 按照输出提示执行下面命令配置 kubeconfig 以便后面我们能正常使用 kubectl 命令\nmkdir -p $HOME/.kube cp -i /etc/kubernetes/admin.conf $HOME/.kube/config chown $(id -u):$(id -g) $HOME/.kube/config 安装网络插件 在安装网络插件之前我们查看一下 node 信息\n# kubectl get nodes NAME STATUS ROLES AGE VERSION yjf-kubeadm NotReady master 4h8m v1.19.16 发现 node 的状态为 NotReady\n通过下面命令安装 flannel 网络插件\n$ sudo kubectl apply -f https://raw.githubusercontent.com/flannel-io/flannel/master/Documentation/kube-flannel.yml podsecuritypolicy.policy/psp.flannel.unprivileged created clusterrole.rbac.authorization.k8s.io/flannel created clusterrolebinding.rbac.authorization.k8s.io/flannel created serviceaccount/flannel created configmap/kube-flannel-cfg created daemonset.apps/kube-flannel-ds created 等一会儿，大概 5 分钟之后在查看 node 信息\n# kubectl get nodes NAME STATUS ROLES AGE VERSION kubeadm Ready master 4h20m v1.19.16 node status 已经变为 ready 了\n添加节点 在新的节点重新执行 安装 docker，安装 kubeadm，节点准备 三个步骤。\n然后执行\nkubeadm join 172.18.30.127:6443 --token abcdef.0123456789abcdef \\ --discovery-token-ca-cert-hash sha256:a86f804bc33460936ce8c6a9bdde774190815fafcd5a093c0d53a8f7bfc72ad3 执行完成后在第一个节点查看 nodes\n# kubectl get nodes -o wide NAME STATUS ROLES AGE VERSION INTERNAL-IP EXTERNAL-IP OS-IMAGE KERNEL-VERSION CONTAINER-RUNTIME kubeadm Ready master 42m v1.19.16 172.18.30.127 \u003cnone\u003e Ubuntu 20.04.3 LTS 5.11.0-34-generic docker://20.10.12 kubeadm2 Ready \u003cnone\u003e 4m43s v1.19.16 172.18.30.151 \u003cnone\u003e Ubuntu 20.04.3 LTS 5.11.0-34-generic docker://20.10.12 ","description":"","tags":null,"title":"在国内使用 kubeadm 部署 k8s","uri":"/documents/blog/kubeadm/"},{"content":"在我们内部，我们称 quic-tun 为弱网神器，尤其是在存在丢包现象的网络环境中， quic-tun 能极大改善 TCP 应用的网络传输。quic-tun 项目包含两个程序，quictun-server 和 quictun-client，其核心原理 可以用一句话概述：quictun-server 把 server 端 TCP/UNIX-SOCKET 应用的传输层协议转为 QUIC， 然后 quictun-client 在把 QUIC 转为 TCP/UNIX-SOCKET，客户端应用程序只需要连接到 quictun-client 服务就可以和 server 端应用程序 交互了。\n通过这种转换，quictun-server 和 quictun-client 之间的网络传输使用的是 QUIC 协议，QUIC 使用了优秀的重传算法和拥塞控制算法，能轻松 应对复杂网络环境。因此对于整体的网络状况会有极大优化。关于 quic-tun 对网络传输的优化，我们做了一些测试，并编写了测试报告， 想了解更多信息，请点击查看。\n架构图 quic-tun 不仅有优化 TCP 传输的作用，他还能把 TCP 应用转为 UNIX-SOCKET 应用，UNIX-SOCKET 应用转为 TCP 应用，其架构图如下：\n使用方法 调整内核参数，增大缓存区，详细原因，请参考官方文档。 server 端和 client 端都要调整这个参数。\nsysctl -w net.core.rmem_max=2500000 打开 release 页面，下载最新版本的 quic-tun，并解压\nwget https://github.com/kungze/quic-tun/releases/download/v0.0.2/quic-tun_0.0.2_linux_amd64.tar.gz tar xvfz quic-tun_0.0.2_linux_amd64.tar.gz 注意：0.0.2 是编写该文档时的最新版本。\n启动 server 端程序\n./quictun-server --listen-on 172.18.31.36:7500 启动客户端程序\n./quictun-client --listen-on tcp:127.0.0.1:6500 --server-endpoint 172.18.31.36:7500 --token-source tcp:172.18.30.117:22 --insecure-skip-verify True 注意：上面参数 --token-source 指定一个 token，这个 token 用于告诉 server 端客户端应用程序想要连接到那个应用程序。后面我们会有更多关于 token 的解释。\n使用 ssh 命令测试\n$ ssh root@127.0.0.1 -p 6500 root@127.0.0.1's password: 可以使用 --help 查看更多 quictun-server 和 quictun-client 支持的更多命令行参数。\n概念解释  tunnel：隧道，quic-tun会为每个 TCP 连接创建一个 tunnel，一个 tunnel 对应一个 QUIC 的 connection（quic-tun 为实现多路复用提出的一个概念）。 client endpoint：隧道的 client 端点，监听在 TCP 端口或者 UNIX-SOCKET 文件，用于接受 client 应用程序的请求并与 server endpoint 建立隧道。 server endpoint：隧道的 server 端点，监听在 UDP(quic) 端口，与 client endpoint 建立隧道后把隧道传输的数据转发到 server 应用。 token：用于告诉 server endpoint，client 应用程序需要连接到哪个 server 应用程序，在 client endpoint 接受 client 应用程序的连接 后第一件事就是生成 token，然后把这个 token 发送到 server endpoint，server endpoint 从这个 token 中解析出 server 应用程序的地址，然后连接到应用 程序，在然后与 client endpoint 建立隧道。quic-tun 提供了很多 token 的获取和解析插件，想了解关于 token 更多信息，请阅读我们专门的章节  ","description":"","tags":null,"title":"quic-tun","uri":"/documents/quic-tun/"},{"content":"","description":"","tags":null,"title":"博文","uri":"/documents/blog/"},{"content":"通用中间件包括 mariadb，rabbitmq，memcached，nginx-ingress-controller 这些中间件我们都通用封装在了 openstack-dep chart 中了。\n部署 password chart 在部署 openstack 的过程中会注册很多数据库和 keystone 用户，对应的我们需要设置很多密码，为了后面部署方便和密 码的安全性，我们专门编写了一个 password chart，部署这个 chart 可以随机生成一些密码，后面我们在部署其他项目时 不用在关心密码问题。\n$ helm -n openstack install openstack-passwork kolla-helm/password NAME: openstack-passwork LAST DEPLOYED: Mon Jun 6 14:15:45 2022 NAMESPACE: openstack STATUS: deployed REVISION: 1 TEST SUITE: None NOTES: CHART NAME: password CHART VERSION: 1.0.0 ** Please be patient while the chart is being deployed ** Check the Secret: kubectl get secret --namespace openstack openstack-passwork -o yaml 部署 openstack-dep openstack-dep 把部署 openstack 所需要的中间件封装在一个 chart 里，通过这种方式后续部署 openstack 其他项目时可以共用这一套中间件：\n mariadb https://github.com/bitnami/charts/tree/master/bitnami/mariadb rabbitmq https://github.com/bitnami/charts/tree/master/bitnami/rabbitmq memcached https://github.com/bitnami/charts/tree/master/bitnami/memcached nginx-ingress-controller https://github.com/bitnami/charts/tree/master/bitnami/nginx-ingress-controller  默认情况下，openstack-dep 会部署 ceph 相关的 configmap。该 configmap 主要存储 ceph 集群的 endpoint 信息，通过指定的 ceph.cephClusterNamespace 和 ceph.cephClusterName 参数来同步 ceph 集群中 endpoint 配置。 为 cinder, glance 等需要使用 ceph 存储的服务，提供 ceph 集群的 endpoint 配置。由于该操作是公共的一次性操作，所以在 openstack-dep 中运行。\nhelm -n openstack install openstack-dependency kolla-helm/openstack-dep \\  --set ceph.cephClusterNamespace=rook-ceph \\  --set ceph.cephClusterName=rook-ceph 这里推荐使用 ceph 做为存储后端，所以默认环境中 ceph.enabled 参数为 true，即默认环境中部署了ceph。 若环境中未部署 ceph，可通过增加 ceph.enabled=false 同时去除 ceph.cephClusterNamespace 和 ceph.cephClusterName 参数来跳过这一步骤。\nhelm -n openstack install openstack-dependency kolla-helm/openstack-dep \\  --set ceph.enabled=false 默认情况下 openstack-dep 会创建一个名称为 openstack NodePort 的 service 向外部暴露 ingerss，设置 externalService.type 为 LoadBalancer 则该 service 会变为 loadbalancer 类型，通过 externalService.loadBalancerIP 可以为这个 service 指定一个特定的 IP，在 外部可以通过这个 IP 访问 openstack 服务。\n通过下面命令观察\nwatch -n 1 kubectl -n openstack get pods -l app.kubernetes.io/instance=openstack-dependency 等待所有的 pod 都 ready 后，安装 openstack keystone。\n","description":"","tags":null,"title":"通用中间件部署","uri":"/documents/kolla-helm/dependency/"},{"content":"keystone 是 openstack 的认证和服务发现组件。\n部署 keystone helm -n openstack install openstack-keystone kolla-helm/keystone 安装 openstackclient 安装 openstack 命令行，在安装完成后可以通过执行 openstack 命令验证安装是否成功。\napt install python3-openstackclient 创建 openstackrc 文件 openstack 认证相关信息会存放在一个专门的 secert （openstack-keystone，与 keystone chart 的 release 名称一致） 中，在 shell 终端执行下面命令导出 OS_* 相关环境变量以便后续 openstack 命令能正常执行。另外特别要注意：openstack 相 关的组件的 API 服务都是通过 service 暴露的，因此要求执行命令的节点能解析 k8s service de 域名（需要安装 nodelocaldns 插件）。\nexport OS_USERNAME=$(kubectl get secret -n openstack openstack-keystone -o jsonpath=\"{.data.OS_USERNAME}\" | base64 --decode) export OS_PROJECT_DOMAIN_NAME=$(kubectl get secret -n openstack openstack-keystone -o jsonpath=\"{.data.OS_PROJECT_DOMAIN_NAME}\" | base64 --decode) export OS_USER_DOMAIN_NAME=$(kubectl get secret -n openstack openstack-keystone -o jsonpath=\"{.data.OS_USER_DOMAIN_NAME}\" | base64 --decode) export OS_PROJECT_NAME=$(kubectl get secret -n openstack openstack-keystone -o jsonpath=\"{.data.OS_PROJECT_NAME}\" | base64 --decode) export OS_REGION_NAME=$(kubectl get secret -n openstack openstack-keystone -o jsonpath=\"{.data.OS_REGION_NAME}\" | base64 --decode) export OS_PASSWORD=$(kubectl get secrets -n openstack openstack-password -o jsonpath=\"{.data.keystone-admin-password}\" | base64 --decode) export OS_AUTH_URL=$(kubectl get secret -n openstack openstack-keystone -o jsonpath=\"{.data.OS_CLUSTER_URL}\" | base64 --decode) export OS_INTERFACE=internal 验证 通过下面命令观察\nwatch -n 1 kubectl -n openstack get pods -l app.kubernetes.io/instance=openstack-keystone 等待所有的 pod 都 ready 后，然后执行下面命令能否执行成功。\n$ source openstackrc $ openstack endpoint list +----------------------------------+-----------+--------------+--------------+---------+-----------+----------------------------------------------------------+ | ID | Region | Service Name | Service Type | Enabled | Interface | URL | +----------------------------------+-----------+--------------+--------------+---------+-----------+----------------------------------------------------------+ | 94e00ea36c2841ca82fd92fe73601870 | RegionOne | keystone | identity | True | public | http://openstack.openstack.svc.cluster.local/identity/v3 | | ba9d6067919f4f86b60afb073538b7ee | RegionOne | keystone | identity | True | admin | http://keystone-api.openstack.svc.cluster.local:5000/v3 | | ea680d2d7362424b8c9715e55516291d | RegionOne | keystone | identity | True | internal | http://keystone-api.openstack.svc.cluster.local:5000/v3 | +----------------------------------+-----------+--------------+--------------+---------+-----------+----------------------------------------------------------+ ","description":"","tags":null,"title":"部署 keystone","uri":"/documents/kolla-helm/keystone/"},{"content":"目前 kolla-helm 支持部署两种 cinder 后端：lvm 和 ceph。但是 lvm 仅用于测试，正式使用推荐 ceph。\n部署 cinder  仅安装 lvm 后端  helm install -n openstack openstack-cinder kolla-helm/cinder --set ceph.enabled=false 注意：lvm 后建议仅用于测试，cinder chart 会创建一个 loop 设备作为 lvm 的 pv 设备，可以通过 参数 lvm.loop_device_directory 和 lvm.loop_device_size 来指定 loop 文件在宿主机上 的存放目录和大小。\n 仅安装 ceph 后端  特别注意的是，ceph 后端依赖 rook，如果想开启 ceph 后端，需要在 部署 cinder 之前安装 rook，并用 rook 创建一个 ceph 集群或使用 rook 纳管一个外部的 ceph 集群。\nhelm install -n openstack openstack-cinder kolla-helm/cinder \\  --set lvm.enabled=false \\  --set ceph.cephClusterNamespace=rook-ceph \\  --set ceph.cephClusterName=rook-ceph ceph.cephClusterNamespace 和 ceph.cephClusterName 需要根据具体情况进行修改，默认情况下如果开启了 ceph 后端，cinder 的 backup 服务也会开启，如果 想关闭，需要设置 ceph.backup.enabled 为 false。\n 同时安装 lvm，ceph 后端  默认情况下（不改变任何参数） lvm 和 ceph 后端同时开启\nhelm install -n openstack openstack-cinder kolla-helm/cinder 验证 通过下面命令观察\nwatch -n 1 kubectl -n openstack get pods -l app.kubernetes.io/instance=openstack-cinder 等待所有的 pod 都 ready 后，创建 volume type 和 volume 检验 cinder 的功能是否正常。\n创建默认 volume type 如果开启了 lvm 后端，默认 volume type 为 lvm；如果只开启了 ceph 后端或者 ceph 和 lvm 后端同时开启， 默认 volume type 为 rbd。\nopenstack volume type create \u003c默认 volume type 名称\u003e 创建卷 cinder create 1 --name vol-test ","description":"","tags":null,"title":"部署 cinder","uri":"/documents/kolla-helm/cinder/"},{"content":"目前 kolla-helm 支持部署两种 glance 后端存储：file 和 ceph。\n部署 glance  使用 file 做为存储后端  helm install -n openstack openstack-glance kolla-helm/glance --set ceph.enabled=false 创建的 image 文件，将存储在 pod 名称为“ glance-api-xxxx” 的 /var/lib/glance/images/ 目录下，默认通过 hostPath 的方式映射到宿主机的 /var/lib/glance/images/ 目录下。\n 使用 ceph 做为存储后端  特别注意的是，ceph 后端依赖 rook，如果想使用 ceph 做为存储后端，需要在\n部署 glance 之前安装 rook，并用 rook 创建一个 ceph 集群或使用 rook 纳管一个外部的 ceph 集群。\nhelm install -n openstack openstack-glance kolla-helm/glance \\  --set ceph.cephClusterNamespace=rook-ceph \\  --set ceph.cephClusterName=rook-ceph ceph.cephClusterNamespace 和 ceph.cephClusterName 需要根据具体情况进行修改，默认情况下是使用 ceph 做为存储后端。\n验证 通过下面命令观察\nwatch -n 1 kubectl -n openstack get pods -l app.kubernetes.io/instance=openstack-glance 等待所有的 pod 都 ready 后，创建 image 检验 glance 的功能是否正常。\n下载 cirros 镜像 wget http://download.cirros-cloud.net/0.3.5/cirros-0.3.5-x86_64-disk.img 创建镜像 openstack image create \"cirros\" \\  --file cirros-0.3.5-x86_64-disk.img \\  --disk-format qcow2 --container-format bare \\  --public 查看镜像 openstack image list +--------------------------------------+-----------+--------+ | ID | Name | Status | +--------------------------------------+-----------+--------+ | 4b525cef-840b-4533-af05-91ae4acfc3f2 | cirros | active | +--------------------------------------+-----------+--------+ ","description":"","tags":null,"title":"部署 glance","uri":"/documents/kolla-helm/glance/"},{"content":"这篇文档会深入介绍 kolla-helm 各个 chart 之间是如何配合的，chart 内部 job 的执行顺序以及如何通过 ingress 向外部暴露 openstack 组件的 api 服务。\n通用 chart 在 koll-helm 有三个 chart 比较特殊：common，password，openstack-dep 这个三个都不是以 openstack 服务命名的，我们称它们为通用 chart。\n  common\ncommon chart 是一个 library chart，包含了很多通用模板（template）和模板函数（template function）,这个 chart 无法直接部署，只能作为其他 chart 的依赖。\n  password\n这个 chart 的作用与 kolla-ansible 的 kolla-genpwd 命令作用类似，password chart 会自动生成部署 openstack 服务所需的 密码（主要是数据库用户密码和 keystone 用户密码），这些密码会存储在一个特定的 secert（名称与 password chart release 名称相同）中。\n  apiVersion:v1data:cinder-database-password:akNMc3hBVWUzWA==cinder-keystone-password:T3FSbnNvTTg2Yg==cinder-rbd-secret-uuid:ZjVhM2ZmNTQtODU3Mi00ZmY3LWI4N2ItM2MzNDcyMDJkNDFkglance-database-password:WlpuTXdOaXdlQQ==glance-keystone-password:WVJWSEhRelFUcw==keystone-admin-password:ajdGNWhGN25hVQ==keystone-database-password:OEI0S2ZCdU5VQg==mariadb-password:dVZCbFE4S2RXaA==mariadb-replication-password:bXJveHZTdjJObg==mariadb-root-password:TEROdVpxakNBYg==neutron-database-password:MUZwMHp3SmVBUw==neutron-keystone-password:ZUlRcWNyNjlibA==nova-database-password:NzF3bWEzODJ4MQ==nova-keystone-password:Q2pKUWJ5VWhsSg==placement-database-password:eEhDWUFlVVRTWg==placement-keystone-password:c2JoNmQxWHpIcg==rabbitmq-password:RXpvdXZFc2t1Wg==rbd-secret-uuid:MjI4ZWQ2M2MtYjAwYy00ZWVkLWE2ZGUtYmMzNTM4NzJkYzMzkind:Secretmetadata:annotations:meta.helm.sh/release-name:openstack-passwordmeta.helm.sh/release-namespace:openstackcreationTimestamp:\"2022-06-06T06:33:11Z\"labels:app.kubernetes.io/managed-by:Helmname:openstack-passwordnamespace:openstackresourceVersion:\"23578343\"uid:f8501cd3-31ab-497e-a0ae-2f93432ab15atype:Opaque  openstack-dep\n这个 chart 主要安装 openstack 组件依赖的通用的服务，如：mariadb，rabbitmq，memcached。还有一个 nginx-ingress-controller， 主要用于统一 openstack 各个服务对外暴露的 endpoint。另外要说明的是这个 chart 仅是封装了 bitnami 提供的对应的 chart，如果想了解各个服务的详细信息和参数请参考 bitnami 文档。\n这个 chart 部署后会生成一个和 release 同名的 secret，这个 secret 中记录了各个服务的连接信息。\n  job 执行顺序 在部署 openstack 服务的过程中需要执行很多 job，如初始化数据库，创建 keystone 用户，创建 loop 设备等，这其中有很多 job 是每个 openstack 服务 都需要执行的，这些 job 的 manifests 模板都放在了 common chart 中，这一章节来介绍一下这些通用 job 的作用，和这些 job 的依赖顺序。\n  *-cm-render\n这个 job 没有任何依赖，通常是最先执行的 job，这个 job 主要是渲染相应的 openstack 服务的配置文件的 configmap。在上文提到了，openstack 服务需 要用到的各种账号的密码都是由 password chart 提前生成并存放的指定的 secert 中的，helm 在渲染模板阶段是无法获取这些密码的，因此在配置文件模板中 需要用到密码的地方我们都放置了一个特殊的字符串占位，这个 job 的作用就是用正确的密码替换这些占位字符串。\n如数据库连接的配置，在执行这个 job 之前如下（以 cinder 为例）：\n[database] connection = mysql+pymysql://cinder:database_password_placeholder@database_endpoint_placeholder/cinder 在执行完这个 job 后相应的占位符被替换：\n[database] connection = mysql+pymysql://cinder:jCLsxAUe3X@openstack-dependency-mariadb.openstack.svc.cluster.local:3306/cinder   *-db-init\n这个 job 在 *-cm-render 之后执行，用于创建相应服务的 database，数据库用户，并授权\n  *-db-sync\n这个 job 在 *-db-init 和 *-cm-render 之后执行，执行相关服务的 db-manage db sync 命名同步表到数据库。\n  *-register\n这个 job 依赖 keystone-api service，只要在 keystone api 接口能正常使用后才执行这个 job，这个 job 用于创建各个服务的 keystone 账号，向 keystone 注册 endpoints。\n  *–update-openstack-conn-info\n在上文我们提到了，部署完 openstack-dep 后生成一个记录各个服务连接信息的 secret 这个 job 就是用来更新这个 secert 的，在我们部署完一个新的 openstack 的服务后 我们需要把这个服务的连接信息写入这个 secert 方便后期其他服务的 chart 使用。\n  ingress 在部署完 openstack-dep 后会默认创建一个名为 openstack-nginx 的 ingressclass，koll-helm 通过创建 ingress 向外暴露 openstack 各个组件的 API 服务，以 cinder 为例，其 manifest 如下\napiVersion:networking.k8s.io/v1kind:Ingressmetadata:annotations:meta.helm.sh/release-name:openstack-cindermeta.helm.sh/release-namespace:openstacknginx.ingress.kubernetes.io/rewrite-target:/$2creationTimestamp:\"2022-06-06T07:44:43Z\"generation:1labels:app.kubernetes.io/managed-by:Helmname:openstack-cindernamespace:openstackresourceVersion:\"23589571\"uid:c93635d3-3815-498b-810a-c4ca0821deb6spec:ingressClassName:openstack-nginxrules:- http:paths:- backend:service:name:cinder-apiport:number:8776path:/volumev3(/|$)(.*)pathType:Prefixstatus:loadBalancer:ingress:- ip:172.18.30.166","description":"","tags":null,"title":"开发文档","uri":"/documents/kolla-helm/developer/"},{"content":"","description":"","tags":null,"title":"Categories","uri":"/documents/categories/"},{"content":"kubeapps 是一个带有 web 界面的 helm chart 管理系统。\n安装 kubeapps helm repo add bitnami https://charts.bitnami.com/bitnami kubectl create namespace kubeapps helm install kubeapps --namespace kubeapps bitnami/kubeapps 创建 service 向外暴露 kubeapps dashboard 如果当前环境支持创建 loadbalancer 类型的 service 则可以用一下 manifest 文件创建 service\napiVersion:v1kind:Servicemetadata:name:kubeapps-dashboardnamespace:kubeappsspec:ports:- name:httpport:80protocol:TCPtargetPort:httpselector:app.kubernetes.io/component:frontendapp.kubernetes.io/instance:kubeappsapp.kubernetes.io/name:kubeappstype:LoadBalancer如果不支持 创建 loadbalancer 类型的 service 则我们可以通过 NodePort 类型的 service 暴露 kubeapps dashboard\napiVersion:v1kind:Servicemetadata:name:kubeapps-dashboardnamespace:kubeappsspec:ports:- name:httpport:80protocol:TCPtargetPort:httpselector:app.kubernetes.io/component:frontendapp.kubernetes.io/instance:kubeappsapp.kubernetes.io/name:kubeappstype:NodePort创建 serviceaccount 并绑定 admin 角色 kubectl create --namespace default serviceaccount kubeapps-operator kubectl create clusterrolebinding kubeapps-operator --clusterrole=cluster-admin --serviceaccount=default:kubeapps-operator cat \u003c\u003cEOF | kubectl apply -f - apiVersion: v1 kind: Secret metadata: name: kubeapps-operator-token namespace: default annotations: kubernetes.io/service-account.name: kubeapps-operator type: kubernetes.io/service-account-token EOF 获取登录 token\nkubectl get --namespace default secret kubeapps-operator-token -o jsonpath='{.data.token}' -o go-template='{{.data.token | base64decode}}' \u0026\u0026 echo 登录 kubeapps dashboard 打开浏览器，输入 http://\u003ckubeapps-dashboard service 地址\u003e，我们可以看到如下界面：\n输入上一步获取的 token，点击 SUBMIT 进入首页\n右上角有两个 default，分别表示 cluster 和 namespace，点开下拉框可以切换 cluster 和 namespace。在往右有九个小方块组成的设置图标，点开可以添加 helm chart 仓库。\n左上方的 Applicatons 会展示当前环境安装的所以 helm chart release，Catalog 列出仓库中所有的 helm chart。\n添加仓库 点击有上方的设置按钮 -\u003e App Repositories -\u003e ADD APP REPOSITORY，会出现下面对话框\n，其中 Name 和 URL 是必填项，其他视情况选填，以 kolla-helm 仓库为例，Name 我们可以填 写 kolla-helm，URL 填写 https://charts.kungze.net。然后点击最下方的 INSTALL REPO 按钮，我们可以看到列表中多了一个新的仓库。\n点击 REFRESH 同步仓库中 chart，点击仓库名称可以看到仓库中都有哪些 chart。\n部署 chart 点击进入我们想要部署的 chart，我们可以看到关于这个 chart 的介绍，右上角有 DEPLOY 按钮，点击进入部署页面，如果 这个 chart 包含 values.schema.json 文件则部署界面会多一个 Form 表单，通过这个表单我们可以定制部署参数，如果 没有 Form 表单我们需要直接修改 YAML 文件来修改配置参数，特别注意上方的 Name 是必填参数，修改完参数后点击下发的 DEPLOY 按钮进行部署，部署完成后会出现部署的应用的详情界面。关注上方的 pod 状态的图标\n所有的 pod 都 Ready 后，这个应用就可以被正常使用了。\n","description":"","tags":null,"title":"kubeapps 安装与使用","uri":"/documents/blog/kubeapps/"},{"content":"kungze Kungze（昆泽）意为昆仑山上的白泽。是卓朗昆仑云团队成立的一个兴趣小组，我们的目的 是向大家分享我们在开发卓朗昆仑云的过程中创建的一些比较有意思的项目。我们在 github 上创建了一个组织开放这些项目的代码。 下面是对这些开源项目的简要介绍：\nK8S openstack 融合 k8s 能弥补 openstack 的哪些不足？\n 简化 openstack 的部署，我们第一阶段的主要工作就是打造一个通过 helm 轻松部署 openstack 的方案 弥补 openstack 应用市场和 DBaaS 的不足 丰富 openstack 日志和监控告警的手段  openstack 能给 k8s 带来哪些好处？\n 借助 openstack cinder 提供多种后端存储 借助 openstack neutron 提供丰富灵活的 CNI 插件 借助 openstack keystone 完善 k8s 租户体系  基于以上的想法我们打算提供一些列的项目。\nkolla-helm 仓库地址：https://github.com/kungze/kolla-helm。这个项目包含一些列 helm chart，基于这些 chart 我们可以很容易的在 k8sk8s 平台上部署 openstackopenstack， 在 openstack 社区有一个 openstack-helm 项目提供了类似的功能，但是由于 openstack-helm 设计过于复杂，项目活跃度底，不兼容最新的 helm v3，所以我们打算提供一套新的 chart。 我们为什么叫 kolla-helm 呢？顾名思义，我们想要借助 kolla 容器来完成新版 chart 的编写。\ncinder-metal-csi 由于 k8s 官方提供的 cinder-csi-plugin 只适用于 k8s on openstack 的场景 (即：k8s 需 要运行在 openstack 虚机里面)。所以我们打算提供一套新的 cinder csi 插件，使运行在裸金属系统上的容器也能 很方便的使用 cinder 存储。这个项目我们还在规划中，目前还没有仓库地址。\nquic-tun 仓库地址：https://github.com/kungze/quic-tun。这个项目原理上和 kcptun 类似，都 是把 TCP 数据包转为 UDP 数据包，借助 UDP 的特性加上一些优秀的重传算法优化流量在不稳定网络（如：网络环境有丢包）上的 传输。但是这两个项目的目的有很大区别。quic-tun 是基于 google 的 quic 协议实现 的（quic 是基于 UDP 的）。其不光有优化网络传输的功能，还有很多其他特性：\n 在服务端仅开启一个端口（需要是 UDP 端口）就可用代理多个服务端应用，在客户端可用通过 token 访问指定的应用 服务端可用代理本地套接字应用程序，在服务端可用把本地套接字流量转为 quic 流量，然后在客户端在转为 TCP 或者本地套接字 加密，quic 是强制有 ssl 加密层的，即使应用程序没有配置 ssl 证书也可用通过 quic-tun 放心的在公网上传输 灵活的 token 控制，可以控制特定的 client 应用程序只连接到特定的 server 应用程序。  一个典型的应用场景：在服务端有多个虚拟机打开了 VNC 或 SPICE，在正常情况下这要求服务器暴露每个虚机的 VNC/SPICE 端口，而且这些端口还是不固定的，这无疑增加了安全风险，加重了安全策略的复杂度。借助 quic-tun，服务器就只需要向外暴露一个端口即可，而且还可以放心的把这个端口暴露在公网上。\n","description":"","tags":null,"title":"Kungze","uri":"/documents/"},{"content":"负载均衡器是一个很重要的 k8s 所依赖的基础设施组件。市场上存在的很多解决方案都是基于 IaaS 的，由 IaaS 平台提供负载均衡器功能；还有一些基于外部负载均衡器设备的的解决方案，这些方案需要购买厂商的设备。这些方案对于 k8s 开发学习人员，还是私有云 paas 平台的实施管理人员都是过重的，成本过高。metallb 就是为在裸金属上部署的 k8s 平台提供一个轻量的外部负载均衡器的软件。metallb 支持两种模式，BGP 模式 和 layer 2 模式。由于 BGP 模式需要有外部路由器配合，所以我们这里介绍更为简单的 layer 2 模式。\n安装 metallb 的安装相对来说比较简短，执行下面两个命令即可\nkubectl apply -f https://raw.githubusercontent.com/metallb/metallb/main/manifests/namespace.yaml kubectl apply -f https://raw.githubusercontent.com/metallb/metallb/main/manifests/metallb.yaml 配置 layer 2 模式 metallb 部署简单，但是存在单点故障，在生产环境建议使用 BGP 模式。\n如果 kube-proxy 使用的是 ipvs 模式，我们首先需要设置 kube-proxy 的 strictARP 为 true。\nmode:\"ipvs\"ipvs:strictARP:true下面是配置 layer 2 模式的示例文件：\napiVersion:v1kind:ConfigMapmetadata:namespace:metallb-systemname:configdata:config:|address-pools: - name: my-ip-space protocol: layer2 addresses: - 192.168.1.240/28我们把上面内容保存在一个文件中，如 config-layer-2.yaml 中，然后我们执行：\nkubectl apply -f config-layer-2.yaml metallb controller 会自动扫描这个 ConfigMap 加载配置。\n特殊的，如果我们没有连续的整段 IP，我们也可以通过下面这种方式配置零散的 IP\napiVersion:v1kind:ConfigMapmetadata:namespace:metallb-systemname:configdata:config:|address-pools: - name: my-ip-space protocol: layer2 addresses: - 192.168.1.240/32 - 192.168.1.250/32 - 192.168.1.243/32使用示例 使用下面的 manifest，我们创建一个 nginx deployment 和 service，service 类型是 LoadBalancer 的\napiVersion:apps/v1kind:Deploymentmetadata:name:nginxspec:selector:matchLabels:app:nginxtemplate:metadata:labels:app:nginxspec:containers:- name:nginximage:nginx:1ports:- name:httpcontainerPort:80---apiVersion:v1kind:Servicemetadata:name:nginxspec:ports:- name:httpport:80protocol:TCPtargetPort:80selector:app:nginxtype:LoadBalancerkubectl apply 执行成功后，执行下面命令查看创建的 service 的信息\n$ kubectl get svc nginx NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE nginx LoadBalancer 10.68.199.238 192.168.1.243 80:30349/TCP 2h 可以看到 EXTERNAL-IP 为 192.168.1.243，打开浏览器输入 http://192.168.1.243，可以成功访问我们刚刚部署的 nginx:\n","description":"","tags":null,"title":"metallb L2 模式部署","uri":"/documents/blog/metallb/"},{"content":"SSL（安全套接字层）在 TCP 协议中是可选的，但是 QUIC 协议中 SSL 是强制存在的。在该章节我们专门介绍 SSL 在 quic-tun 中的应用。 该文档默认您已经掌握 SSL 涉及到的概念：公钥，私钥，证书，CA。SSL 的功能主要有两个：数据加密、认证。该文档主要针对第二点，介绍 一下 SSL 认证功能在 quic-tun 中的应用。\n相关参数  --ca-file 指定 CA 证书，在 quictun-client 需要是为 quictun-server 的公钥签名的 CA 的证书；在 quictun-server 需 要是为 quictun-client 的公钥签名的证书 --cert-file 指定证书文件（经过 CA 签名的公钥） --key-file 指定私钥文件 --verify-client 这个参数是 quictun-server 特有的，用于指定 quictun-server 是否验证 quictun-client，如果该值为 True，那 么 quictun-client 启动时必须有 --cert-file 和 --key-file 参数。 --insecure-skip-verify 这个参数是 quictun-client 特有的，用于指定是否验证 quictun-server 的证书，这个值为 False 时表明需要 验证 quictun-server 的证书，这时候 quictun-client 的 --ca-file 参数是必须要传的。  QUIC 协议中加密是强制要求的，但是认证是可选的。而加密工作依赖于服务端的的私钥和证书，在 quic-tun 中指定私钥和证书的参数分别 是 --key-file 和 --cert-file。如果在启动 quictun-server 是不指定这两个参数，quictun-server 会自动创建一个临时的 私钥和证书并进行自签名。\n","description":"","tags":null,"title":"SSL","uri":"/documents/quic-tun/ssl/"},{"content":"","description":"","tags":null,"title":"Tags","uri":"/documents/tags/"},{"content":"token 在 quic-tun 中有两个作用：1、告诉 server endpoint，准备新建的隧道需要连接的 应用程序的地址；2、用于 server endpoint 判断是否接受隧道创建请求。在 quictun-client我们 提供了一些 token 获取插件，在 quictun-server 我们提供了一些 token 的解析插件。\nToken 获取插件 token 的获取工作是在quictun-client中完成的，在 quictun-client 我们提供了两个命令行参数：--token-source-plugin 和 --token-source 用来在启动时加载指定的 token 获取插件。--token-source-plugin 用于指定插件名称，--token-source 的 参数值会作为插件初始化时的参数。目前我们支持的 token 获取插件如下：\nFixed 固定 token，这个插件一个 quictun-client 只有一个 token，这也就意味着一个 quictun-client 服务只能 为一个固定的 server 应用程序建立隧道，也就是说所有连接到这个 quictun-client 服务的 client 应用程序都 会连接到一个固定的 server 服务。\n示例：\n./quictun-client --listen-on tcp:127.0.0.1:6500 --server-endpoint 172.18.31.36:7500 --token-source-plugin Fixed --token-source tcp:172.18.30.117:22 --insecure-skip-verify True 注意：Fixed 是 quictun-clint 默认的 token source plugin，如果你不知道则默认使用这个 plugin。\nFile 这个 plugin 会从一个外部文件中读取 token，文件内容格式如下：\n172.26.106.191 tcp:10.20.30.5:2256 172.26.106.192 tcp:10.20.30.6:3306 172.26.106.193 tcp:10.20.30.6:3306 文件中每行空格前面的是 client 应用程序所在机器的 IP 地址，空格后面的是 token（这里 token 是明文 token，直接展示了 server 应用程序的地址）\n这种方式，当 quictun-client 接收 client 应用程序的连接时会首先提取 client 应用程序 的应用地址，然后从文件中读取 token，在与 quictun-server 建立隧道。 这种方法不同的 client 应用程序可以连接到不同的 server 应用程序。\n示例：\n./quictun-client --insecure-skip-verify --server-endpoint 127.0.0.1:7500 --token-source-plugin File --token-source /etc/quictun/tokenfile --listen-on tcp:172.18.31.36:6622 这里 --token-source 就用于指定存放 token 的外部文件的路径。\nToken 解析插件 token 的解析是在 quictun-server 中完成的，quictun-server 的两个参数 --token-parser-plugin 和 --token-parser-key 用于指定 解析插件，及其参数。目前我们只实现了一个解析插件 Cleartext。\nCleartext 根据字面意思理解这是一个明文的解析插件，也就是说 quictun-client 传过来的 token 没有经过加密，quictun-server 不解密不认证这个 token，直接使用。 这种方法在内部使用是可以的，但是在公网环境，或者 quictun-client 所在环境不受信的情况下这个 plugin 可能会有一些安全隐患（泄露 server 应用程序的信息）。 我们也正在计划实现一种加密 token 的解析插件。\n示例：\n./quictun-server --listen-on 172.18.31.36:7500 --token-parser-plugin Cleartext --token-parser-key base64 --token-parser-key 指定了 token 的编码方式，上面示例中 token 使用了 base64 编码。那么 quictun-client 传过来的 token 必须是经过 base64 编码的：\n./quictun-client --listen-on tcp:127.0.0.1:6500 --server-endpoint 172.18.31.36:7500 --token-source-plugin Fixed --token-source dGNwOjE3Mi4xOC4zMC4xMTc6MjI= --insecure-skip-verify True 注意上面的 --token-source 参数 dGNwOjE3Mi4xOC4zMC4xMTc6MjI= 就是 tcp:172.18.30.117:22 经过 base64 编码之后生成的字符串。\n","description":"","tags":null,"title":"token","uri":"/documents/quic-tun/token/"},{"content":"准备工作   在 server 虚机\n启动 iperf3 server\n$ iperf3 -s ----------------------------------------------------------- Server listening on 5201 ----------------------------------------------------------- 打开一个新的终端，启动该 quictun-server\n$ ./quictun-server I0624 09:15:29.223140 1515 server.go:30] \"Server endpoint start up successful\" listen address=\"[::]:7500\"   在 client 虚机\n启动 quictun-server\n$ ./quictun-client --server-endpoint 192.168.26.129:7500 --token-source tcp:127.0.0.1:5201 --insecure-skip-verify true I0624 09:17:30.926905 1679 client.go:35] \"Client endpoint start up successful\" listen address=\"127.0.0.1:6500\" 上面的 192.168.26.129 是 server 虚机的 IP 地址。\n打开一个新的终端，执行 iperf3 -c 开始测试\n  直接测试 TCP 的性能\niperf3 -c 192.168.26.129 -p 5201 -t 20   测试通过 quic-tun 转发的性能\niperf3 -c 127.0.0.1 -p 6500 -t 20   打开一个新的终端，执行 top 命令观察 CPU 使用情况\ntop   测试结果 为了更清楚的展示结果，我们只记录了 client 的截屏\n不设置丢包率（丢包率为 0.0%）  TCP   quic-tun  丢包率设置为 0.1%  TCP   quic-tun  丢包率设置为 0.5%  TCP   quic-tun  丢包率设置为 1.0%  TCP   quic-tun  丢包率设置为 5.0%  TCP   quic-tun  汇总  TCP     丢包率 (%) sender bitRate(Mbits/sec) reveiver bitRate(Mbits/sec) CPU(ksoftirqd)     0 562 561 18.7   0.1 307 306 10.0   0.5 80.2 79.9 1.0   1 60.7 60.3 2.0   5 13.0 12.8 1.3     quic-tun     丢包率 (%) sender bitRate(Mbits/sec) reveiver bitRate(Mbits/sec) CPU(ksoftirqd) CPU(quictun-client)     0 604 601 2.7 75.1   0.1 348 344 2.0 66.3   0.5 239 235 1.0 49.8   1 138 135 1.0 35.7   5 38.8 36.4 1.3 22.9    从上面的测试结果我们可以看出即使没有设置丢包率，quic-tun 也能提升一定的网络带宽，在存在丢包的网络环境中，quic-tun 的表现像 比于 TCP 直接传输有了巨大的提升。但是在带宽较大的场景，quic-tun 占用了较多的 CPU（但是软中断占用的 CPU 降到了很低）\n火焰图 为了分析为什么 quic-tun 在大带宽下占用如此多的 CPU，我们制作了 CPU 火焰图：\n从上面的火焰图可以看出，内核空间的 udp_sendmsg 占用了较多的 CPU，在网上查阅文档发现下面这篇文档可以解释这种现象而且文章也给出了优化方案。\nhttps://conferences.sigcomm.org/sigcomm/2020/files/slides/epiq/0%20QUIC%20and%20HTTP_3%20CPU%20Performance.pdf\n","description":"","tags":null,"title":"带宽和CPU使用率","uri":"/documents/quic-tun/performance-test/bandwidth-and-cpu/"},{"content":"为了验证 quic-tun 对网络传输性能的影响，我们做了一些测试，并编写了这个测试报告。测试过程中我们 主要关注三个指标：传输带宽，网络延迟，CPU 使用率。在测试过程中我们构造了不同丢包率的网络环境， 然后对比不同丢包率的网络环境下：TCP 直接传输和通过 quic-tun 转换之后传输这两种情况下前面三 个指标的不同。\n测试环境和测试工具  两个 VMware workstation 虚机，一个作为 server 端，一个作为 client 端，后面简称 server 或者 client 一个公网虚机，这个用于测试 quic-tun 在公网环境的表现。公网虚机作为 server 端，位于阿里云华东区，clien 端是一个 VMware 虚机，测试时本人位于天津 操作系统：ubuntu 20.02 带宽测试工具：iperf3 测试过程中使用 top 命令 观察 CPU 情况  对于网络延迟，我没有找到一个比较好的测试工具，因此我自己用 python 脚本编写了 一个测试工具。\n如何设置不同的丢包率 我们使用 VMware workstation 自带的功能来设置不同的丢包率，设置方法如下：\n在测试过程中我们仅改变了 client 的丢包率，server 没有设置丢包率，并且 传入和传出丢包率设置一致。\n","description":"","tags":null,"title":"性能测试报告","uri":"/documents/quic-tun/performance-test/"},{"content":"在这个测试场景中，我们将额外测试公网环境下，quic-tun 对网络延迟的影响。 测试工具是 我们针对这个场景专门编写的。\n准备工作   在 server 虚机\n启动 TCP server\n$ ./server The server listen on 0.0.0.0:15676 打开一个新的终端，启动 quictun-server\n$ ./quictun-server I0624 09:15:29.223140 1515 server.go:30] \"Server endpoint start up successful\" listen address=\"[::]:7500\"   在 client 虚机\n启动 quictun-client\n$ ./quictun-client --server-endpoint 192.168.26.129:7500 --token-source tcp:127.0.0.1:5201 --insecure-skip-verify true I0624 09:17:30.926905 1679 client.go:35] \"Client endpoint start up successful\" listen address=\"127.0.0.1:6500\" 192.168.26.129 是 server 虚机的 IP 地址，在公网场景，这个就是公网 IP。\n打开一个新的终端，运行 client python 脚本测试网络延迟\n 测试直接 TCP 传输的网络延迟  ./client --server-host 192.168.26.131  测试通过 quic-tun 传输的网络延迟  ./client --server-host 127.0.0.1 --server-port 6500   测试结果 丢包率设置为 0.0% (本地网络)  TCP  $ ./client --server-host 192.168.26.131 First packet latency: 0.7572174072265625 ms Total latency: 499.89843368530273 ms  quic-tun  $ ./client --server-host 127.0.0.1 --server-port 6500 First packet latency: 7.899284362792969 ms Total latency: 591.1564826965332 ms 丢包率设置为 1.0% (本地网络)  TCP  $ ./client --server-host 192.168.26.131 First packet latency: 0.6091594696044922 ms Total latency: 4290.04430770874 ms  quic-tun  $ ./client --server-host 127.0.0.1 --server-port 6500 First packet latency: 8.286714553833008 ms Total latency: 1201.0939121246338 ms 丢包率设置为 0.0% (公网网络)  TCP  $ ./client --server-host 47.111.149.1 --server-port 5201 First packet latency: 24.95884895324707 ms Total latency: 25493.980407714844 ms  quic-tun  $ ./client --server-host 127.0.0.1 --server-port 6500 First packet latency: 100.67296028137207 ms Total latency: 24987.539291381836 ms 丢包率设置为 1.0% (公网网络)  TCP  $ ./client --server-host 47.111.149.1 --server-port 5201 First packet latency: 23.789167404174805 ms Total latency: 28489.194869995117 ms  quic-tun  $ ./client --server-host 127.0.0.1 --server-port 6500 First packet latency: 103.04689407348633 ms Total latency: 27247.18403816223 ms 丢包率设置为 2.0% (公网网络)  TCP  $ ./client --server-host 47.111.149.1 --server-port 5201 First packet latency: 36.420583724975586 ms Total latency: 33720.38745880127 ms  quic-tun  $ ./client --server-host 127.0.0.1 --server-port 6500 First packet latency: 100.87323188781738 ms Total latency: 27528.08117866516 ms 结果分析 从上面的测试结果可以看出在本地存在丢包的网络环境中，quic-tun 对改善网络延迟有突出的表现，没有丢包是改善不明显。 在公网环境中，quic-tun 对网络延迟的改善并不明显。另外需要特别注意的是在使用 quic-tun 的情况下第一个包的网络延迟 特别的长，这是因为 quic-tun 隧道的建立是由第一个包触发的，第一个包的传输需要等待隧道建立成功。从这里也可以看出 quic-tun 不适合短连接 TCP 应用，如频繁且短暂的 http 请求应用，但是对于长链接的 http 应用我相信 quic-tun 也能对 其有很大优化。\n","description":"","tags":null,"title":"网络延迟","uri":"/documents/quic-tun/performance-test/network-latency/"}]