[{"content":"kolla-helm 包含一些列 helm chart，基于这些 chart 我们可以很容易的在 k8s 平台上部署 openstack，在 openstack 社区有一个 openstack-helm 项目 提供了类似的功能，但是由于 openstack-helm 设计过于复杂，项目活跃度底，不兼容最新的 helm v3，所以我们打算提供一套新的 chart。 我们为什么叫 kolla-helm 呢？顾名思义，我们想要借助 kolla 容器来完成新版 chart 的编写。 借助这些 chart 你可以有多种方式来轻松的部署 openstack\n通过命令行部署 你可以通过 helm 命令部署 openstack。\n图形化部署 你还可以通过 kubeapp 提供的 web UI 轻松的部署 openstack。\n该目录下面的文档时以命令行部署的方式展示如何通过 kolla-helm 部署 openstack。\n","description":"","tags":null,"title":"kolla-helm","uri":"/documents/kolla-helm/"},{"content":"部署 k8s 首先我们需要有一套 k8s 集群，k8s 集群可以有多种方式，由于国内网络环境的问题，我们推荐两个可以在国内丝滑部署 k8s 集群的方案\n kubeadm，可以参考我们的文档在国内使用 kubeadm 部署 k8s kubeasz，这是由国内大神编写的一套部署 k8s 的 ansible 脚本，可以满足各种需要，使用方式可以参考这个项目的文档  需要特别注意的是：为了能在宿主机节点上解析 k8s service 的域名（方便在宿主机上执行 openstack 命令）， nodelocaldns 是强烈推荐安装的。\n部署 metallb 我们提供的方案需要在裸金属上直接部署 k8s，所以我们需要部署 metallb 为 loadbalancer 类型的 service 提供支撑，部署方式可以参考文档。 另外 openelb 也是一个不错的方案，但是 openelb 作者没有部署过，感兴趣的可以自己找文档研究部署。\n部署 rook（可选） rook 是一个开源的云原生存储编排平台，可以用来管理 ceph 集群，当我们 设置 cinder，glance，nova 的后端存储为 ceph 时，我们先必须要部署 rook，并使用 rook 创建创建一个 ceph 集群 或者纳管一个外部的 ceph 集群。\nceph 集群（可选） 我们可以参考官方文档 使用 rook 创建 一套 ceph 集群，但是在生产环境中我们更推荐使用传统先行部署一套 ceph 集群，然后再用 rook 纳管已经部署好的 ceph 集群。ceph 集群的部署可以使用 ceph-ansible，rook 纳管已经存在的 ceph 集群的方法可以参考文档.\n安装 helm helm 的安装就比较简单了，可以参考官方文档，而且只用在执行部署命令的节点安装就行。 另外还可以安装 kubeapps 来为我们提供图形界面部署 openstack。kubeapps 的安装 文档可以参考我们专门写的一篇博客。\n添加 helm 仓库 helm repo add kolla-helm https://kungze.github.io/kolla-helm 在国内访问 github 可能会失败，你可以使用我们在国内的备份仓库\nhelm repo add kolla-helm https://charts.kungze.net 创建 namespace（可选） 建议单独创建一个 k8s namespace 用于部署 openstack 相关的 chart。\nkubectl create namespace openstack 在准备工作完成后我们可以正式部署 openstack 了，注意：password，openstack-dep 和 keystone 需要优先部署。\n","description":"","tags":null,"title":"准备工作","uri":"/documents/kolla-helm/preparation/"},{"content":"kubeadm 是 k8s 官方提供的一个部署管理 k8s 集群的工具，但是 kubeadm 使用到的很多资源的下载地址都在国外，如果按照官方文档操作很容易因为网络原因失败。这里基于 ubuntu 20.04 展示如何让 kubeadm 使用国内的资源部署 k8s 集群。\n下面所有命令都是使用 root 用户执行的\n安装 docker 参照 docker 官方安装文档\n$ curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo gpg --dearmor -o /usr/share/keyrings/docker-archive-keyring.gpg $ echo \\ \"deb [arch=$(dpkg --print-architecture) signed-by=/usr/share/keyrings/docker-archive-keyring.gpg] https://download.docker.com/linux/ubuntu \\ $(lsb_release -cs) stable\" | sudo tee /etc/apt/sources.list.d/docker.list \u003e /dev/null $ apt-get update $ apt-get install docker-ce docker-ce-cli containerd.io 通过阿里源安装：\n$ curl -fsSL https://mirrors.aliyun.com/docker-ce/linux/ubuntu/gpg | sudo gpg --dearmor -o /usr/share/keyrings/docker-archive-keyring.gpg $ echo \\ \"deb [arch=amd64 signed-by=/usr/share/keyrings/docker-archive-keyring.gpg] https://mirrors.aliyun.com/docker-ce/linux/ubuntu \\ $(lsb_release -cs) stable\" | sudo tee /etc/apt/sources.list.d/docker.list \u003e /dev/null $ apt-get update $ apt-get install docker-ce docker-ce-cli containerd.io 安装 kubeadm 官方文档使用的 https://apt.kubernetes.io/ apt 源在国内被屏蔽了，所以我们需要找一个国内的镜像源，这里我们以阿里源为例\napt-get install -y apt-transport-https curl https://mirrors.aliyun.com/kubernetes/apt/doc/apt-key.gpg | apt-key add - cat \u003c\u003cEOF \u003e/etc/apt/sources.list.d/kubernetes.list deb https://mirrors.aliyun.com/kubernetes/apt/ kubernetes-xenial main EOF apt update 在安装之前可以通过下面命令我们可以安装的 kubeadm 的版本\napt-cache madison kubeadm 根据情况选择适合的版本安装\napt-get install kubectl=1.23.7-00 kubelet=1.23.7-00 kubeadm=1.23.7-00 注意：这里选择的版本决定了最终安装的 k8s 的版本，从 1.24 开始 k8s 从代码中彻底移除了 dockershim，所以我们这里选择用 k8s 1.23.7，如果想在 k8s 1.24 及以后使用 docker 作为 cri，则需要安装一个外部的 dockershim，mirantis 的 cri-dockerd 是一个不错的选择。\n节点准备 关闭 swap 分区 ( 貌似最新的 k8s 版本不在要求这一步 )\nsudo swapoff -a # 暂时关闭，永久关闭可以上网查询 初始化环境 创建一个 yaml 文件，我们命名为 kubeadm-init-config.yaml，填入以下内容\n---apiVersion:kubeadm.k8s.io/v1beta3kind:InitConfigurationbootstrapTokens:- token:abcdef.0123456789abcdefttl:24h0m0slocalAPIEndpoint:advertiseAddress:172.18.30.127bindPort:6443nodeRegistration:criSocket:/var/run/dockershim.sockimagePullPolicy:IfNotPresenttaints:[]---apiVersion:kubeadm.k8s.io/v1beta3kind:ClusterConfigurationapiServer:timeoutForControlPlane:4m0scertificatesDir:/etc/kubernetes/pkiclusterName:kubernetescontrollerManager:{}dns:{}etcd:local:dataDir:/var/lib/etcdimageRepository:registry.aliyuncs.com/google_containerskubernetesVersion:1.23.0networking:dnsDomain:cluster.localserviceSubnet:10.96.0.0/12podSubnet:10.244.0.0/16scheduler:{}---apiVersion:kubelet.config.k8s.io/v1beta1kind:KubeletConfigurationfailSwapOn:falseaddress:0.0.0.0enableServer:truecgroupDriver:cgroupfs---apiVersion:kubeproxy.config.k8s.io/v1alpha1kind:KubeProxyConfigurationmode:ipvsipvs:strictARP:true这里有几个参数要特别注意一下：\n advertiseAddress：这个要改为当前主机的管理网 IP 地址 imageRepository：这是一个关键的配置，从国内源下载相关镜像 podSubnet: 这个是下面部署的 flannel cni 插件要求的一个参数，需要和 flannel 网络配置 net-conf.json 中的 Network 保持一致 cgroupDriver: 这个需要和 docker 的 Cgroup Driver 保持一致，可以通过命令 docker info|grep \"Cgroup Driver\" 查看  然后执行：\nsudo kubeadm init --config kubeadm-init-config.yaml 如果你不想关闭 swap 分区，使用下面命令初始化\nsudo kubeadm init --config kubeadm-init-config.yaml --ignore-preflight-errors Swap 等待一会儿，初始化成功后会获得如下输出：\nYour Kubernetes control-plane has initialized successfully! To start using your cluster, you need to run the following as a regular user: mkdir -p $HOME/.kube sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config sudo chown $(id -u):$(id -g) $HOME/.kube/config Alternatively, if you are the root user, you can run: export KUBECONFIG=/etc/kubernetes/admin.conf You should now deploy a pod network to the cluster. Run \"kubectl apply -f [podnetwork].yaml\" with one of the options listed at: https://kubernetes.io/docs/concepts/cluster-administration/addons/ Then you can join any number of worker nodes by running the following on each as root: kubeadm join 172.18.30.127:6443 --token abcdef.0123456789abcdef \\ --discovery-token-ca-cert-hash sha256:159072d62927901183f5cda29fbb4e110ee8e354a350aad7774239e19c57169a 按照输出提示执行下面命令配置 kubeconfig 以便后面我们能正常使用 kubectl 命令\nmkdir -p $HOME/.kube cp -i /etc/kubernetes/admin.conf $HOME/.kube/config chown $(id -u):$(id -g) $HOME/.kube/config 安装网络插件 在安装网络插件之前我们查看一下 node 信息\n# kubectl get nodes NAME STATUS ROLES AGE VERSION yjf-kubeadm NotReady master 4h8m v1.19.16 发现 node 的状态为 NotReady\n通过下面命令安装 flannel 网络插件\n$ sudo kubectl apply -f https://raw.githubusercontent.com/flannel-io/flannel/master/Documentation/kube-flannel.yml podsecuritypolicy.policy/psp.flannel.unprivileged created clusterrole.rbac.authorization.k8s.io/flannel created clusterrolebinding.rbac.authorization.k8s.io/flannel created serviceaccount/flannel created configmap/kube-flannel-cfg created daemonset.apps/kube-flannel-ds created 等一会儿，大概 5 分钟之后在查看 node 信息\n# kubectl get nodes NAME STATUS ROLES AGE VERSION kubeadm Ready master 4h20m v1.19.16 node status 已经变为 ready 了\n添加节点 在新的节点重新执行 安装 docker，安装 kubeadm，节点准备 三个步骤。\n然后执行\nkubeadm join 172.18.30.127:6443 --token abcdef.0123456789abcdef \\ --discovery-token-ca-cert-hash sha256:a86f804bc33460936ce8c6a9bdde774190815fafcd5a093c0d53a8f7bfc72ad3 执行完成后在第一个节点查看 nodes\n# kubectl get nodes -o wide NAME STATUS ROLES AGE VERSION INTERNAL-IP EXTERNAL-IP OS-IMAGE KERNEL-VERSION CONTAINER-RUNTIME kubeadm Ready master 42m v1.19.16 172.18.30.127 \u003cnone\u003e Ubuntu 20.04.3 LTS 5.11.0-34-generic docker://20.10.12 kubeadm2 Ready \u003cnone\u003e 4m43s v1.19.16 172.18.30.151 \u003cnone\u003e Ubuntu 20.04.3 LTS 5.11.0-34-generic docker://20.10.12 ","description":"","tags":null,"title":"在国内使用 kubeadm 部署 k8s","uri":"/documents/blog/kubeadm/"},{"content":"","description":"","tags":null,"title":"博文","uri":"/documents/blog/"},{"content":"通用中间件包括 mariadb，rabbitmq，memcached，nginx-ingress-controller 这些中间件我们都通用封装在了 openstack-dep chart 中了。\n部署 password chart 在部署 openstack 的过程中会注册很多数据库和 keystone 用户，对应的我们需要设置很多密码，为了后面部署方便和密 码的安全性，我们专门编写了一个 password chart，部署这个 chart 可以随机生成一些密码，后面我们在部署其他项目时 不用在关心密码问题。\n$ helm -n openstack install openstack-passwork kolla-helm/password NAME: openstack-passwork LAST DEPLOYED: Mon Jun 6 14:15:45 2022 NAMESPACE: openstack STATUS: deployed REVISION: 1 TEST SUITE: None NOTES: CHART NAME: password CHART VERSION: 1.0.0 ** Please be patient while the chart is being deployed ** Check the Secret: kubectl get secret --namespace openstack openstack-passwork -o yaml 部署 openstack-dep openstack-dep 把部署 openstack 所需要的中间件封装在一个 chart 里，通过这种方式后续部署 openstack 其他项目时可以共用这一套中间件：\n mariadb https://github.com/bitnami/charts/tree/master/bitnami/mariadb rabbitmq https://github.com/bitnami/charts/tree/master/bitnami/rabbitmq memcached https://github.com/bitnami/charts/tree/master/bitnami/memcached nginx-ingress-controller https://github.com/bitnami/charts/tree/master/bitnami/nginx-ingress-controller  helm -n openstack install openstack-dependency kolla-helm/openstack-dep 默认情况下 openstack-dep 会创建一个名称为 openstack NodePort 的 service 向外部暴露 ingerss，设置 externalService.type 为 LoadBalancer 则该 service 会变为 loadbalancer 类型，通过 externalService.loadBalancerIP 可以为这个 service 指定一个特定的 IP，在 外部可以通过这个 IP 访问 openstack 服务。\n通过下面命令观察\nwatch -n 1 kubectl -n openstack get pods -l app.kubernetes.io/instance=openstack-dependency 等待所有的 pod 都 ready 后，安装 openstack keystone。\n","description":"","tags":null,"title":"通用中间件部署","uri":"/documents/kolla-helm/dependency/"},{"content":"keystone 是 openstack 的认证和服务发现组件。\n部署 keystone helm -n openstack install openstack-keystone kolla-helm/keystone 安装 openstackclient 安装 openstack 命令行，在安装完成后可以通过执行 openstack 命令验证安装是否成功。\napt install python3-openstackclient 创建 openstackrc 文件 openstack 认证相关信息会存放在一个专门的 secert （openstack-keystone，与 keystone chart 的 release 名称一致） 中，在 shell 终端执行下面命令导出 OS_* 相关环境变量以便后续 openstack 命令能正常执行。另外特别要注意：openstack 相 关的组件的 API 服务都是通过 service 暴露的，因此要求执行命令的节点能解析 k8s service de 域名（需要安装 nodelocaldns 插件）。\nexport OS_USERNAME=$(kubectl get secret -n openstack openstack-keystone -o jsonpath=\"{.data.OS_USERNAME}\" | base64 --decode) export OS_PROJECT_DOMAIN_NAME=$(kubectl get secret -n openstack openstack-keystone -o jsonpath=\"{.data.OS_PROJECT_DOMAIN_NAME}\" | base64 --decode) export OS_USER_DOMAIN_NAME=$(kubectl get secret -n openstack openstack-keystone -o jsonpath=\"{.data.OS_USER_DOMAIN_NAME}\" | base64 --decode) export OS_PROJECT_NAME=$(kubectl get secret -n openstack openstack-keystone -o jsonpath=\"{.data.OS_PROJECT_NAME}\" | base64 --decode) export OS_REGION_NAME=$(kubectl get secret -n openstack openstack-keystone -o jsonpath=\"{.data.OS_REGION_NAME}\" | base64 --decode) export OS_PASSWORD=$(kubectl get secrets -n openstack openstack-password -o jsonpath=\"{.data.keystone-admin-password}\" | base64 --decode) export OS_AUTH_URL=$(kubectl get secret -n openstack openstack-keystone -o jsonpath=\"{.data.OS_CLUSTER_URL}\" | base64 --decode) export OS_INTERFACE=internal 验证 通过下面命令观察\nwatch -n 1 kubectl -n openstack get pods -l app.kubernetes.io/instance=openstack-keystone 等待所有的 pod 都 ready 后，然后执行下面命令能否执行成功。\n$ source openstackrc $ openstack endpoint list +----------------------------------+-----------+--------------+--------------+---------+-----------+----------------------------------------------------------+ | ID | Region | Service Name | Service Type | Enabled | Interface | URL | +----------------------------------+-----------+--------------+--------------+---------+-----------+----------------------------------------------------------+ | 94e00ea36c2841ca82fd92fe73601870 | RegionOne | keystone | identity | True | public | http://openstack.openstack.svc.cluster.local/identity/v3 | | ba9d6067919f4f86b60afb073538b7ee | RegionOne | keystone | identity | True | admin | http://keystone-api.openstack.svc.cluster.local:5000/v3 | | ea680d2d7362424b8c9715e55516291d | RegionOne | keystone | identity | True | internal | http://keystone-api.openstack.svc.cluster.local:5000/v3 | +----------------------------------+-----------+--------------+--------------+---------+-----------+----------------------------------------------------------+ ","description":"","tags":null,"title":"部署 keystone","uri":"/documents/kolla-helm/keystone/"},{"content":"目前 kolla-helm 支持部署两种 cinder 后端：lvm 和 ceph。但是 lvm 仅用于测试，正式使用推荐 ceph。\n部署 cinder  仅安装 lvm 后端  helm install -n openstack openstack-cinder kolla-helm/cinder --set ceph.enabled=false 注意：lvm 后建议仅用于测试，cinder chart 会创建一个 loop 设备作为 lvm 的 pv 设备，可以通过 参数 lvm.loop_device_directory 和 lvm.loop_device_size 来指定 loop 文件在宿主机上 的存放目录和大小。\n 仅安装 ceph 后端  特别注意的是，ceph 后端依赖 rook，如果想开启 ceph 后端，需要在 部署 cinder 之前安装 rook，并用 rook 创建一个 ceph 集群或使用 rook 纳管一个外部的 ceph 集群。\nhelm install -n openstack openstack-cinder kolla-helm/cinder \\  --set lvm.enabled=false \\  --set ceph.cephClusterNamespace=rook-ceph \\  --set ceph.cephClusterName=rook-ceph ceph.cephClusterNamespace 和 ceph.cephClusterName 需要根据具体情况进行修改，默认情况下如果开启了 ceph 后端，cinder 的 backup 服务也会开启，如果 想关闭，需要设置 ceph.backup.enabled 为 false。\n 同时安装 lvm，ceph 后端  默认情况下（不改变任何参数） lvm 和 ceph 后端同时开启\nhelm install -n openstack openstack-cinder kolla-helm/cinder 验证 通过下面命令观察\nwatch -n 1 kubectl -n openstack get pods -l app.kubernetes.io/instance=openstack-cinder 等待所有的 pod 都 ready 后，创建 volume type 和 volume 检验 cinder 的功能是否正常。\n创建默认 volume type 如果开启了 lvm 后端，默认 volume type 为 lvm；如果只开启了 ceph 后端或者 ceph 和 lvm 后端同时开启， 默认 volume type 为 rbd。\nopenstack volume type create \u003c默认 volume type 名称\u003e 创建卷 cinder create 1 --name vol-test ","description":"","tags":null,"title":"部署 cinder","uri":"/documents/kolla-helm/cinder/"},{"content":"这篇文档会深入介绍 kolla-helm 各个 chart 之间是如何配合的，chart 内部 job 的执行顺序以及如何通过 ingress 向外部暴露 openstack 组件的 api 服务。\n通用 chart 在 koll-helm 有三个 chart 比较特殊：common，password，openstack-dep 这个三个都不是以 openstack 服务命名的，我们称它们为通用 chart。\n  common\ncommon chart 是一个 library chart，包含了很多通用模板（template）和模板函数（template function）,这个 chart 无法直接部署，只能作为其他 chart 的依赖。\n  password\n这个 chart 的作用与 kolla-ansible 的 kolla-genpwd 命令作用类似，password chart 会自动生成部署 openstack 服务所需的 密码（主要是数据库用户密码和 keystone 用户密码），这些密码会存储在一个特定的 secert（名称与 password chart release 名称相同）中。\n  apiVersion:v1data:cinder-database-password:akNMc3hBVWUzWA==cinder-keystone-password:T3FSbnNvTTg2Yg==cinder-rbd-secret-uuid:ZjVhM2ZmNTQtODU3Mi00ZmY3LWI4N2ItM2MzNDcyMDJkNDFkglance-database-password:WlpuTXdOaXdlQQ==glance-keystone-password:WVJWSEhRelFUcw==keystone-admin-password:ajdGNWhGN25hVQ==keystone-database-password:OEI0S2ZCdU5VQg==mariadb-password:dVZCbFE4S2RXaA==mariadb-replication-password:bXJveHZTdjJObg==mariadb-root-password:TEROdVpxakNBYg==neutron-database-password:MUZwMHp3SmVBUw==neutron-keystone-password:ZUlRcWNyNjlibA==nova-database-password:NzF3bWEzODJ4MQ==nova-keystone-password:Q2pKUWJ5VWhsSg==placement-database-password:eEhDWUFlVVRTWg==placement-keystone-password:c2JoNmQxWHpIcg==rabbitmq-password:RXpvdXZFc2t1Wg==rbd-secret-uuid:MjI4ZWQ2M2MtYjAwYy00ZWVkLWE2ZGUtYmMzNTM4NzJkYzMzkind:Secretmetadata:annotations:meta.helm.sh/release-name:openstack-passwordmeta.helm.sh/release-namespace:openstackcreationTimestamp:\"2022-06-06T06:33:11Z\"labels:app.kubernetes.io/managed-by:Helmname:openstack-passwordnamespace:openstackresourceVersion:\"23578343\"uid:f8501cd3-31ab-497e-a0ae-2f93432ab15atype:Opaque  openstack-dep\n这个 chart 主要安装 openstack 组件依赖的通用的服务，如：mariadb，rabbitmq，memcached。还有一个 nginx-ingress-controller， 主要用于统一 openstack 各个服务对外暴露的 endpoint。另外要说明的是这个 chart 仅是封装了 bitnami 提供的对应的 chart，如果想了解各个服务的详细信息和参数请参考 bitnami 文档。\n这个 chart 部署后会生成一个和 release 同名的 secret，这个 secret 中记录了各个服务的连接信息。\n  job 执行顺序 在部署 openstack 服务的过程中需要执行很多 job，如初始化数据库，创建 keystone 用户，创建 loop 设备等，这其中有很多 job 是每个 openstack 服务 都需要执行的，这些 job 的 manifests 模板都放在了 common chart 中，这一章节来介绍一下这些通用 job 的作用，和这些 job 的依赖顺序。\n  *-cm-render\n这个 job 没有任何依赖，通常是最先执行的 job，这个 job 主要是渲染相应的 openstack 服务的配置文件的 configmap。在上文提到了，openstack 服务需 要用到的各种账号的密码都是由 password chart 提前生成并存放的指定的 secert 中的，helm 在渲染模板阶段是无法获取这些密码的，因此在配置文件模板中 需要用到密码的地方我们都放置了一个特殊的字符串占位，这个 job 的作用就是用正确的密码替换这些占位字符串。\n如数据库连接的配置，在执行这个 job 之前如下（以 cinder 为例）：\n[database] connection = mysql+pymysql://cinder:database_password_placeholder@database_endpoint_placeholder/cinder 在执行完这个 job 后相应的占位符被替换：\n[database] connection = mysql+pymysql://cinder:jCLsxAUe3X@openstack-dependency-mariadb.openstack.svc.cluster.local:3306/cinder   *-db-init\n这个 job 在 *-cm-render 之后执行，用于创建相应服务的 database，数据库用户，并授权\n  *-db-sync\n这个 job 在 *-db-init 和 *-cm-render 之后执行，执行相关服务的 db-manage db sync 命名同步表到数据库。\n  *-register\n这个 job 依赖 keystone-api service，只要在 keystone api 接口能正常使用后才执行这个 job，这个 job 用于创建各个服务的 keystone 账号，向 keystone 注册 endpoints。\n  *–update-openstack-conn-info\n在上文我们提到了，部署完 openstack-dep 后生成一个记录各个服务连接信息的 secret 这个 job 就是用来更新这个 secert 的，在我们部署完一个新的 openstack 的服务后 我们需要把这个服务的连接信息写入这个 secert 方便后期其他服务的 chart 使用。\n  ingress 在部署完 openstack-dep 后会默认创建一个名为 openstack-nginx 的 ingressclass，koll-helm 通过创建 ingress 向外暴露 openstack 各个组件的 API 服务，以 cinder 为例，其 manifest 如下\napiVersion:networking.k8s.io/v1kind:Ingressmetadata:annotations:meta.helm.sh/release-name:openstack-cindermeta.helm.sh/release-namespace:openstacknginx.ingress.kubernetes.io/rewrite-target:/$2creationTimestamp:\"2022-06-06T07:44:43Z\"generation:1labels:app.kubernetes.io/managed-by:Helmname:openstack-cindernamespace:openstackresourceVersion:\"23589571\"uid:c93635d3-3815-498b-810a-c4ca0821deb6spec:ingressClassName:openstack-nginxrules:- http:paths:- backend:service:name:cinder-apiport:number:8776path:/volumev3(/|$)(.*)pathType:Prefixstatus:loadBalancer:ingress:- ip:172.18.30.166","description":"","tags":null,"title":"开发文档","uri":"/documents/kolla-helm/developer/"},{"content":"","description":"","tags":null,"title":"Categories","uri":"/documents/categories/"},{"content":"kubeapps 是一个带有 web 界面的 helm chart 管理系统。\n安装 kubeapps helm repo add bitnami https://charts.bitnami.com/bitnami kubectl create namespace kubeapps helm install kubeapps --namespace kubeapps bitnami/kubeapps 创建 service 向外暴露 kubeapps dashboard 如果当前环境支持创建 loadbalancer 类型的 service 则可以用一下 manifest 文件创建 service\napiVersion:v1kind:Servicemetadata:name:kubeapps-dashboardnamespace:kubeappsspec:ports:- name:httpport:80protocol:TCPtargetPort:httpselector:app.kubernetes.io/component:frontendapp.kubernetes.io/instance:kubeappsapp.kubernetes.io/name:kubeappstype:LoadBalancer如果不支持 创建 loadbalancer 类型的 service 则我们可以通过 NodePort 类型的 service 暴露 kubeapps dashboard\napiVersion:v1kind:Servicemetadata:name:kubeapps-dashboardnamespace:kubeappsspec:ports:- name:httpport:80protocol:TCPtargetPort:httpselector:app.kubernetes.io/component:frontendapp.kubernetes.io/instance:kubeappsapp.kubernetes.io/name:kubeappstype:NodePort创建 serviceaccount 并绑定 admin 角色 kubectl create --namespace default serviceaccount kubeapps-operator kubectl create clusterrolebinding kubeapps-operator --clusterrole=cluster-admin --serviceaccount=default:kubeapps-operator cat \u003c\u003cEOF | kubectl apply -f - apiVersion: v1 kind: Secret metadata: name: kubeapps-operator-token namespace: default annotations: kubernetes.io/service-account.name: kubeapps-operator type: kubernetes.io/service-account-token EOF 获取登录 token\nkubectl get --namespace default secret kubeapps-operator-token -o jsonpath='{.data.token}' -o go-template='{{.data.token | base64decode}}' \u0026\u0026 echo 登录 kubeapps dashboard 打开浏览器，输入 http://\u003ckubeapps-dashboard service 地址\u003e，我们可以看到如下界面：\n输入上一步获取的 token，点击 SUBMIT 进入首页\n右上角有两个 default，分别表示 cluster 和 namespace，点开下拉框可以切换 cluster 和 namespace。在往右有九个小方块组成的设置图标，点开可以添加 helm chart 仓库。\n左上方的 Applicatons 会展示当前环境安装的所以 helm chart release，Catalog 列出仓库中所有的 helm chart。\n添加仓库 点击有上方的设置按钮 -\u003e App Repositories -\u003e ADD APP REPOSITORY，会出现下面对话框\n，其中 Name 和 URL 是必填项，其他视情况选填，以 kolla-helm 仓库为例，Name 我们可以填 写 kolla-helm，URL 填写 https://charts.kungze.net。然后点击最下方的 INSTALL REPO 按钮，我们可以看到列表中多了一个新的仓库。\n点击 REFRESH 同步仓库中 chart，点击仓库名称可以看到仓库中都有哪些 chart。\n部署 chart 点击进入我们想要部署的 chart，我们可以看到关于这个 chart 的介绍，右上角有 DEPLOY 按钮，点击进入部署页面，如果 这个 chart 包含 values.schema.json 文件则部署界面会多一个 Form 表单，通过这个表单我们可以定制部署参数，如果 没有 Form 表单我们需要直接修改 YAML 文件来修改配置参数，特别注意上方的 Name 是必填参数，修改完参数后点击下发的 DEPLOY 按钮进行部署，部署完成后会出现部署的应用的详情界面。关注上方的 pod 状态的图标\n所有的 pod 都 Ready 后，这个应用就可以被正常使用了。\n","description":"","tags":null,"title":"kubeapps 安装与使用","uri":"/documents/blog/kubeapps/"},{"content":"kungze Kungze 是由一群从事云计算方面工作小伙伴组建的一个兴趣小组，我们的目的是想把我们的一些脑洞变成可用的项目并提供给大家使用，当前我们的工作主要聚焦在 kubernetes 和 openstack，当前的目标是打造一套 kubernetes 和 openstack 混合交付方案 (我们不想 提供一套 k8s on openstack 的方案，我相信这类方案市场上有很多了)，在我们的部署方案中 kubernetes 和 openstack 是平级的，结 合两者的长处打造一个稳定，高效，灵活且易于部署的云平台。\nK8S openstack 融合 k8s 能弥补 openstack 的哪些不足？\n 简化 openstack 的部署，我们第一阶段的主要工作就是打造一个通过 helm 轻松部署 openstack 的方案 弥补 openstack 应用市场和 DBaaS 的不足 丰富 openstack 日志和监控告警的手段  openstack 能给 k8s 带来哪些好处？\n 借助 openstack cinder 提供多种后端存储 借助 openstack neutron 提供丰富灵活的 CNI 插件 借助 openstack keystone 完善 k8s 租户体系  基于以上的想法我们打算提供一些列的工具。\nkolla-helm 仓库地址：https://github.com/kungze/kolla-helm。这个项目包含一些列 helm chart，基于这些 chart 我们可以很容易的在 k8s 平台上部署 openstack， 在 openstack 社区有一个 openstack-helm 项目提供了类似的功能，但是由于 openstack-helm 设计过于复杂，项目活跃度底，不兼容最新的 helm v3，所以我们打算提供一套新的 chart。 我们为什么叫 kolla-helm 呢？顾名思义，我们想要借助 kolla 容器来完成新版 chart 的编写。\ncinder-metal-csi 由于 k8s 官方提供的 cinder-csi-plugin 只适用于 k8s on openstack 的场景 (即：k8s 需 要运行在 openstack 虚机里面)。所以我们打算提供一套新的 cinder csi 插件，使运行在裸金属系统上的容器也能 很方便的使用 cinder 存储。这个项目我们还在规划中，目前还没有仓库地址。\n其他奇奇怪怪的项目 除了我们的主线任务：k8s 与 openstack 融合所需的项目。我们还有一些成员脑洞的一些项目，这些项目可能在某些特殊的场景有一些意想不到的用途。\nquic-tun 仓库地址：https://github.com/kungze/quic-tun。这个项目原理上和 kcptun 类似，都是把 TCP 数据包转为 UDP 数据包，借助 UDP 的特性加上一些优秀的重传算法优化流量在公网上的传输。但是这两个项目的目的有很大区别。quic-tun 是基于 google 的 quic 协议实现的（quic 是基于 UDP 的）。其不光有优化网络传输的功能，还有很大其他特性：\n 在服务端仅开启一个端口（需要是 UDP 端口）就可用代理多个服务端应用，在客户端可用通过 token 访问指定的应用 服务端可用代理本地套接字应用程序，在服务端可用把本地套接字流量转为 quic 流量，然后在客户端在转为 TCP 或者本地套接字 加密，quic 是强制有 ssl 加密层的，即使应用程序没有配置 ssl 证书也可用通过 quic-tun 放心的在公网上传输  一个典型的应用场景：在服务端有多个虚拟机打开了 VNC 或 SPICE，在正常情况下这要求服务器暴露每个虚机的 VNC/SPICE 端口，而且这些端口还是不固定的，这无疑增加了安全风险，加重了安全策略的复杂度。借助 quic-tun，服务器就只需要向外暴露一个端口即可，而且还可以放心的把这个端口暴露在公网上。\n","description":"","tags":null,"title":"Kungze","uri":"/documents/"},{"content":"负载均衡器是一个很重要的 k8s 所依赖的基础设施组件。市场上存在的很多解决方案都是基于 IaaS 的，由 IaaS 平台提供负载均衡器功能；还有一些基于外部负载均衡器设备的的解决方案，这些方案需要购买厂商的设备。这些方案对于 k8s 开发学习人员，还是私有云 paas 平台的实施管理人员都是过重的，成本过高。metallb 就是为在裸金属上部署的 k8s 平台提供一个轻量的外部负载均衡器的软件。metallb 支持两种模式，BGP 模式 和 layer 2 模式。由于 BGP 模式需要有外部路由器配合，所以我们这里介绍更为简单的 layer 2 模式。\n安装 metallb 的安装相对来说比较简短，执行下面两个命令即可\nkubectl apply -f https://raw.githubusercontent.com/metallb/metallb/main/manifests/namespace.yaml kubectl apply -f https://raw.githubusercontent.com/metallb/metallb/main/manifests/metallb.yaml 配置 layer 2 模式 metallb 部署简单，但是存在单点故障，在生产环境建议使用 BGP 模式。\n如果 kube-proxy 使用的是 ipvs 模式，我们首先需要设置 kube-proxy 的 strictARP 为 true。\nmode:\"ipvs\"ipvs:strictARP:true下面是配置 layer 2 模式的示例文件：\napiVersion:v1kind:ConfigMapmetadata:namespace:metallb-systemname:configdata:config:|address-pools: - name: my-ip-space protocol: layer2 addresses: - 192.168.1.240/28我们把上面内容保存在一个文件中，如 config-layer-2.yaml 中，然后我们执行：\nkubectl apply -f config-layer-2.yaml metallb controller 会自动扫描这个 ConfigMap 加载配置。\n特殊的，如果我们没有连续的整段 IP，我们也可以通过下面这种方式配置零散的 IP\napiVersion:v1kind:ConfigMapmetadata:namespace:metallb-systemname:configdata:config:|address-pools: - name: my-ip-space protocol: layer2 addresses: - 192.168.1.240/32 - 192.168.1.250/32 - 192.168.1.243/32使用示例 使用下面的 manifest，我们创建一个 nginx deployment 和 service，service 类型是 LoadBalancer 的\napiVersion:apps/v1kind:Deploymentmetadata:name:nginxspec:selector:matchLabels:app:nginxtemplate:metadata:labels:app:nginxspec:containers:- name:nginximage:nginx:1ports:- name:httpcontainerPort:80---apiVersion:v1kind:Servicemetadata:name:nginxspec:ports:- name:httpport:80protocol:TCPtargetPort:80selector:app:nginxtype:LoadBalancerkubectl apply 执行成功后，执行下面命令查看创建的 service 的信息\n$ kubectl get svc nginx NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE nginx LoadBalancer 10.68.199.238 192.168.1.243 80:30349/TCP 2h 可以看到 EXTERNAL-IP 为 192.168.1.243，打开浏览器输入 http://192.168.1.243，可以成功访问我们刚刚部署的 nginx:\n","description":"","tags":null,"title":"metallb L2 模式部署","uri":"/documents/blog/metallb/"},{"content":"","description":"","tags":null,"title":"Tags","uri":"/documents/tags/"}]